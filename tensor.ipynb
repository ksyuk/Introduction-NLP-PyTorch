{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6653e2-6438-4975-b35b-ce6e13ed79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data', exist_ok=True)\n",
    "# dataディレクトリにAG_NEWSデータセットをダウンロード\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c99f7f-699c-4f2d-99dc-7591f3951827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer: 文章を単語に分割する\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d5d3559-82ba-4028-bce8-cd57a31ce483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_sentence = train_dataset[0][1]\n",
    "# f_tokens = tokenizer(first_sentence)\n",
    "# 29 ['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fea48b9-faa4-4cf5-910c-15652998e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット内の各単語の出現回数をカウント\n",
    "counter = collections.Counter()\n",
    "for _, line in train_dataset:\n",
    "    counter.update(tokenizer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a36ccd0a-78cd-428e-9dfa-47a0a7a4d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab: データセットに出現する単語を単語IDとして持つ。\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
    "# [[0, 'wall'], [1, 'st'], [2, '.'], [3, 'bears'], [4, 'claw'], [5, 'back'], [6, 'into'], [7, 'the'], [8, 'black'], [9, '('], [10, 'reuters'], [11, ')'], [10, 'reuters'], [12, '-'], [13, 'short-sellers'], [14, ','], [0, 'wall'], [15, 'street'], [16, \"'\"], [17, 's'], [18, 'dwindling\\\\band'], [19, 'of'], [20, 'ultra-cynics'], [14, ','], [21, 'are'], [22, 'seeing'], [23, 'green'], [24, 'again'], [2, '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return [vocab.get_stoi()[s] for s in tokenizer(x)]\n",
    "\n",
    "# 文章中の単語を単語IDに変換\n",
    "# vec = encode(first_sentence)\n",
    "# wall -> 0\n",
    "# st -> 1\n",
    "# . -> 2\n",
    "# bears -> 3\n",
    "# claw -> 4\n",
    "# back -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    return [vocab.get_itos()[i] for i in x]\n",
    "\n",
    "# 単語IDを単語に変換\n",
    "# decoded_vec = decode(vec)\n",
    "# 0 -> wall\n",
    "# 1 -> st\n",
    "# 2 -> .\n",
    "# 3 -> bears\n",
    "# 4 -> claw\n",
    "# 5 -> back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "# 'hot dog'という単語列の意味と'hot'と'dog'の単語の意味は意味が全く異なる。\n",
    "# そこで、bivocab(N-grameでn=2)が全ての単語のペアを格納する\n",
    "\n",
    "def create_bi_vocab(dataset):\n",
    "    bi_counter = collections.Counter()\n",
    "    for _, line in dataset:\n",
    "        bi_counter.update(ngrams_iterator(tokenizer(line), ngrams=2))\n",
    "    bi_vocab = torchtext.vocab.vocab(bi_counter, min_freq=2)\n",
    "    return bi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語IDに変換\n",
    "def encode_ngram(texts, vocab):\n",
    "    bi_vocabs = []\n",
    "    for s in tokenizer(texts):\n",
    "        if counter[s] == 1:\n",
    "            continue\n",
    "        bi_vocabs.append(vocab.get_stoi()[s])\n",
    "    return bi_vocabs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
