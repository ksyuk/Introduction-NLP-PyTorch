{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tensor import train_dataset, encode, vocab, classes\n",
    "from bow import train_epoch\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(batch):\n",
    "    # batch is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "\n",
    "    # build vectorized sequence\n",
    "    vectors = [encode(x[1]) for x in batch]\n",
    "\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len, vectors))\n",
    "\n",
    "    labels = []\n",
    "    for t in batch:\n",
    "        labels.append(t[0]-1)\n",
    "    features = []\n",
    "    for t in vectors:\n",
    "        features.append(torch.nn.functional.pad(torch.tensor(t), (0, l-len(t)), mode='constant', value=0))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor(labels),\n",
    "        torch.stack(features)\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "network = EmbedClassifier(vocab_size, 32 ,len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(network, train_loader, learning_rate=1, epoch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(batch):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in batch]\n",
    "\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "\n",
    "    labels = []\n",
    "    for t in batch:\n",
    "        labels.append(t[0]-1)\n",
    "\n",
    "    return ( \n",
    "        torch.LongTensor(labels), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = EmbedClassifier(vocab_size, 32, len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(network, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None, report_freq=200):\n",
    "    print(\"traing...\")\n",
    "\n",
    "    optimizer = optimizer or torch.optim.Adam(network.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    network.train()\n",
    "\n",
    "    total_loss, accurancy, count, i = 0, 0, 0, 0\n",
    "    for labels, text, off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels, text, off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = network(text, off)\n",
    "        loss = loss_fn(out, labels) #cross_entropy(out,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _, predicted = torch.max(out,1)\n",
    "        acuurancy += (predicted==labels).sum()\n",
    "        count += len(labels)\n",
    "\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acuurancy={accurancy.item()/count}\")\n",
    "\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "\n",
    "    return total_loss.item()/count, accurancy.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_emb(network, train_loader, lr=4, epoch_size=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
